{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----------------+\n",
      "|label|Feature|binarezed_Feature|\n",
      "+-----+-------+-----------------+\n",
      "|0    |0.4    |0.0              |\n",
      "|1    |0.5    |0.0              |\n",
      "|2    |0.9    |1.0              |\n",
      "+-----+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SQLContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from numpy import array\n",
    "sc = SparkContext('local')\n",
    "spark = SQLContext(sc)\n",
    "continusdf = spark.createDataFrame([\n",
    "    (0,0.4),\n",
    "    (1,0.5),\n",
    "    (2,0.9)],[\"label\",\"Feature\"])\n",
    "binarizer = Binarizer(threshold=0.5,inputCol=\"Feature\",outputCol=\"binarezed_Feature\")\n",
    "binarizerdf = binarizer.transform(continusdf)\n",
    "binarizerdf.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|Features|bucketed Feature|\n",
      "+--------+----------------+\n",
      "|  -123.4|             0.0|\n",
      "|    -0.5|             1.0|\n",
      "|    -0.4|             1.0|\n",
      "|     0.0|             2.0|\n",
      "|     0.1|             2.0|\n",
      "|     1.0|             3.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SQLContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "# 也就是说SparkContext是Spark的入口，相当于应用程序的main函数。\n",
    "# 目前在一个JVM进程中可以创建多个SparkContext，但是只能有一个active级别的。\n",
    "# 如果你需要创建一个新的SparkContext实例，必须先调用stop方法停掉当前active级别的\n",
    "# SparkContext实例。\n",
    "sc = SparkContext('local')\n",
    "# Spark SQL程序的主入口是SQLContext类或它的子类。\n",
    "# 创建一个基本的SQLContext，你只需要SparkContext\n",
    "spark = SQLContext(sc)\n",
    "# float(\"-inf\")负无穷—-0.5\n",
    "#-0.5—0.0\n",
    "#0.0—0.5\n",
    "#0.5—2.0\n",
    "splits = [float(\"-inf\"),-0.5,0.0,0.5,2.0]\n",
    "data = [(-123.4,),(-0.5,),(-0.4,),(0.0,),(0.1,),(1.0,)]\n",
    "# Features 传数据给分箱器\n",
    "dataFrame = spark.createDataFrame(data,['Features'])\n",
    "#根据前边设定的值分割数据\n",
    "bucketizer = Bucketizer(splits=splits,inputCol='Features',outputCol='bucketed Feature')\n",
    "bucketedData = bucketizer.transform(dataFrame)\n",
    "bucketedData.show()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-f0cad12d686e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0msplits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mfenxiang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshuzu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfenxiang\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0msplits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshuzu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mshendu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuzu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "from pyspark import SQLContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "import numpy as np\n",
    "\n",
    "sc = SparkContext('local')\n",
    "spark = SQLContext(sc)\n",
    "data = [(-123.4,),(-0.5,),(-0.4,),(0.0,),(0.1,),(1.0,)]\n",
    "data.sort()\n",
    "shuzu = np.array(data)\n",
    "fenxiang = 3\n",
    "shendu = (shuzu[len(data)-1] - shuzu[0])/3.0\n",
    "splits = []\n",
    "fenxiang = len(shuzu)/3\n",
    "for i in fenxiang:\n",
    "    splits.add(shuzu[0] + i*shendu)\n",
    "b = shuzu.tolist()\n",
    "dataFrame = spark.createDataFrame(b,['Features'])\n",
    "bucketizer = Bucketizer(splits=splits,inputCol='Features',outputCol='bucketed Feature')\n",
    "bucketedData = bucketizer.transform(dataFrame)\n",
    "bucketedData.show()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-123.4,), (-0.5,), (-0.4,), (0.0,), (0.1,), (1.0,)]\n"
     ]
    }
   ],
   "source": [
    "data = [(-123.4,),(-0.4,),(0.0,),(0.1,),(1.0,),(-0.5,)]\n",
    "data.sort()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuzu = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shuzu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-123.4,), (-0.5,), (-0.4,), (0.0,), (0.1,), (1.0,)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SQLContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "import numpy as np\n",
    "\n",
    "sc = SparkContext('local')\n",
    "spark = SQLContext(sc)\n",
    "data = [(-123.4,),(-0.5,),(-0.4,),(0.0,),(0.1,),(1.0,)]\n",
    "data.sort()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array() missing required argument 'object' (pos 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-7ae0d0ebb1c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgeshu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshuzu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mgeshu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msplits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshuzu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: array() missing required argument 'object' (pos 1)"
     ]
    }
   ],
   "source": [
    "shuzu = np.array(data)\n",
    "geshu = 3\n",
    "b = len(shuzu)/geshu\n",
    "splits = np.array()\n",
    "for i in range(len(shuzu)):\n",
    "    if (i+1) % b == 0:\n",
    "        splits.add(shuzu[i])\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "data = [(-123.4,),(-0.5,),(-0.4,),(0.0,),(0.1,),(1.0,)]\n",
    "date_sort=data.sort()\n",
    "print(date_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
