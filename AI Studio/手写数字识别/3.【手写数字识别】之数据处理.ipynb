{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"PaddlePaddle 1.7.2 (Python 3.5)","language":"python","name":"py35-paddle1.2.0"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"3.【手写数字识别】之数据处理.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01d--XTGg2qA","executionInfo":{"status":"ok","timestamp":1617501429073,"user_tz":-480,"elapsed":8005,"user":{"displayName":"Ming Di","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhiYdtLRyWxEL5-a0628SLf-QrwCseeizEKVqj=s64","userId":"00101951631288157688"}},"outputId":"472b4567-c1c3-496e-ad12-badb8d71b5f2"},"source":["!python3 -m pip --version # 查看pip版本\n","!python -m pip install --upgrade pip #更新pip版本\n","!python -m pip install paddlepaddle==1.7.2 -i https://mirror.baidu.com/pypi/simple #下载paddle1.7.2版本\n","# !python -m pip install paddlepaddle==2.0.1 -i https://mirror.baidu.com/pypi/simple"],"execution_count":null,"outputs":[{"output_type":"stream","text":["pip 21.0.1 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.0.1)\n","Looking in indexes: https://mirror.baidu.com/pypi/simple\n","Requirement already satisfied: paddlepaddle==1.7.2 in /usr/local/lib/python3.7/dist-packages (1.7.2)\n","Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (2.1.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (4.4.2)\n","Requirement already satisfied: rarfile in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (4.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (4.1.2.30)\n","Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (1.19.5)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (3.2.5)\n","Requirement already satisfied: funcsigs in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (1.0.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (3.13)\n","Requirement already satisfied: scipy<=1.3.1 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (1.3.1)\n","Requirement already satisfied: objgraph in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (3.5.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (3.2.2)\n","Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (3.12.4)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (0.10.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (7.1.2)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from paddlepaddle==1.7.2) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.1.0->paddlepaddle==1.7.2) (54.2.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle==1.7.2) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle==1.7.2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle==1.7.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle==1.7.2) (2020.12.5)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->paddlepaddle==1.7.2) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->paddlepaddle==1.7.2) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->paddlepaddle==1.7.2) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->paddlepaddle==1.7.2) (2.4.7)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->paddlepaddle==1.7.2) (0.2.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable->paddlepaddle==1.7.2) (3.8.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable->paddlepaddle==1.7.2) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable->paddlepaddle==1.7.2) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"kW7SN6Nsg1l0"},"source":["# 概述\n","\n","上一节我们使用“横纵式”教学法中的纵向极简方案快速完成手写数字识别任务的建模，但模型测试效果并未达成预期。我们换个思路，从横向展开，如 **图1** 所示，逐个环节优化，以达到最优训练效果。本节主要介绍手写数字识别模型中，数据处理的优化方法。\n","\n","<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/57e18e98b40f4b4c9d9b6c0ef3cf3feee8f62bdd12f844ff8959f766643b9c59\" width=\"800\" hegiht=\"\" ></center>\n","<center><br>图1：“横纵式”教学法 — 数据处理优化 </br></center>\n","<br></br>\n","\n","上一节，我们通过调用飞桨提供的API（[paddle.dataset.mnist](https://www.paddlepaddle.org.cn/documentation/docs/zh/api_cn/data_cn/dataset_cn/mnist_cn.html)）加载MNIST数据集。但在工业实践中，我们面临的任务和数据环境千差万别，通常需要自己编写适合当前任务的数据处理程序，一般涉及如下五个环节：\n","\n","* 读入数据\n","* 划分数据集\n","* 生成批次数据\n","* 训练样本集乱序\n","* 校验数据有效性\n","\n","### 前提条件\n","\n","在数据读取与处理前，首先要加载飞桨平台和数据处理库，代码如下。"]},{"cell_type":"code","metadata":{"id":"g_TAATdAg1l7"},"source":["#数据处理部分之前的代码，加入部分数据处理的库\n","import paddle\n","import paddle.fluid as fluid\n","from paddle.fluid.dygraph.nn import Linear\n","import numpy as np\n","import os\n","import gzip\n","import json\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"X8oJPCuNg1l8"},"source":["# 读入数据并划分数据集\n","\n","在实际应用中，保存到本地的数据存储格式多种多样，如MNIST数据集以json格式存储在本地，其数据存储结构如 **图2** 所示。\n","\n","<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/7d278024d7ac4d6689fdbe0aa1729181699444730e3941d386a55a1ff8ab4276\" width=\"600\" hegiht=\"\" ></center>\n","<center><br>图2：MNIST数据集的存储结构</br></center>\n","<br></br>\n","\n","**data**包含三个元素的列表：**train_set**、**val_set**、 **test_set**。\n","\n","* **train_set（训练集）**：包含50000条手写数字图片和对应的标签，用于确定模型参数。\n","* **val_set（验证集）**：包含10000条手写数字图片和对应的标签，用于调节模型超参数（如多个网络结构、正则化权重的最优选择）。\n","* **test_set（测试集）**：包含10000条手写数字图片和对应的标签，用于估计应用效果（没有在模型中应用过的数据，更贴近模型在真实场景应用的效果）。\n","\n","**train_set**包含两个元素的列表：**train_images**、**train_labels**。\n","\n","* **train_imgs**：[50000, 784]的二维列表，包含5000张图片。每张图片用一个长度为784的向量表示，内容是28\\*28尺寸的像素灰度值（黑白图片）。\n","* **train_labels**：[50000, ]的列表，表示这些图片对应的分类标签，即0-9之间的一个数字。\n","\n","在本地`./work/`目录下读取文件名称为`mnist.json.gz`的MINST数据，并拆分成训练集、验证集和测试集，实现方法如下所示。\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-fpVrkfg1l9","executionInfo":{"status":"ok","timestamp":1617501616084,"user_tz":-480,"elapsed":15738,"user":{"displayName":"Ming Di","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhiYdtLRyWxEL5-a0628SLf-QrwCseeizEKVqj=s64","userId":"00101951631288157688"}},"outputId":"249cb15e-80af-4ee0-cf84-50894ac8be4d"},"source":["# 声明数据集文件位置\n","datafile = '/content/drive/MyDrive/AI Studio/json/mnist.json.gz'\n","print('loading mnist dataset from {} ......'.format(datafile))\n","# 加载json数据文件\n","data = json.load(gzip.open(datafile))\n","print('mnist dataset load done')\n","# 读取到的数据区分训练集，验证集，测试集\n","train_set, val_set, eval_set = data\n","\n","# 数据集相关参数，图片高度IMG_ROWS, 图片宽度IMG_COLS\n","IMG_ROWS = 28\n","IMG_COLS = 28\n","\n","# 打印数据信息\n","imgs, labels = train_set[0], train_set[1]\n","print(\"训练数据集数量: \", len(imgs))\n","\n","# 观察验证集数量\n","imgs, labels = val_set[0], val_set[1]\n","print(\"验证数据集数量: \", len(imgs))\n","\n","# 观察测试集数量\n","imgs, labels = val= eval_set[0], eval_set[1]\n","print(\"测试数据集数量: \", len(imgs))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading mnist dataset from /content/drive/MyDrive/AI Studio/json/mnist.json.gz ......\n","mnist dataset load done\n","训练数据集数量:  50000\n","验证数据集数量:  10000\n","测试数据集数量:  10000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"JDeW2QZ2g1l-"},"source":["## 扩展阅读：为什么学术界的模型总在不断精进呢？\n","\n","通常某组织发布一个新任务的训练集和测试集数据后，全世界的科学家都针对该数据集进行创新研究，随后大量针对该数据集的论文会陆续发表。论文1的A模型声称在测试集的准确率70%，论文2的B模型声称在测试集的准确率提高到72%，论文N的X模型声称在测试集的准确率提高到90% ...\n","\n","然而这些论文中的模型在测试集上准确率提升真实有效么？我们不妨大胆猜测一下。\n","\n","假设所有论文共产生1000个模型，这些模型使用的是测试数据集来评判模型效果，并最终选出效果最优的模型。这相当于把原始的测试集当作了验证集，使得测试集失去了真实评判模型效果的能力，正如机器学习领域非常流行的一句话：“拷问数据足够久，它终究会招供”。\n","\n","<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/c3745dc9aed9499f99c97264141310183f92ac33ef864570bf08f13551963ad1\" width=\"400\" hegiht=\"\" ></center>\n","<center><br>图3：拷问数据足够久，它总会招供</br></center>\n","<br></br>\n","\n","那么当我们需要将学术界研发的模型复用于工业项目时，应该如何选择呢？给读者一个小建议：当几个模型的准确率在测试集上差距不大时，尽量选择网络结构相对简单的模型。往往越精巧设计的模型和方法，越不容易在不同的数据集之间迁移。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"I2a_w1Vlg1l-"},"source":["# 训练样本乱序、生成批次数据\n","\n","* **训练样本乱序：** 先将样本按顺序进行编号，建立ID集合index_list。然后将index_list乱序，最后按乱序后的顺序读取数据。\n","\n","------\n","**说明：**\n","\n","通过大量实验发现，模型对最后出现的数据印象更加深刻。训练数据导入后，越接近模型训练结束，最后几个批次数据对模型参数的影响越大。为了避免模型记忆影响训练效果，需要进行样本乱序操作。\n","\n","------\n","* **生成批次数据：** 先设置合理的batch_size，再将数据转变成符合模型输入要求的np.array格式返回。同时，在返回数据时将Python生成器设置为``yield``模式，以减少内存占用。\n","\n","在执行如上两个操作之前，需要先将数据处理代码封装成load_data函数，方便后续调用。load_data有三种模型：``train``、``valid``、``eval``，分为对应返回的数据是训练集、验证集、测试集。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fWOlJFbQoTcb","executionInfo":{"status":"ok","timestamp":1617501963364,"user_tz":-480,"elapsed":1173,"user":{"displayName":"Ming Di","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhiYdtLRyWxEL5-a0628SLf-QrwCseeizEKVqj=s64","userId":"00101951631288157688"}},"outputId":"1da139bb-8b5f-48ff-b66d-cd5b7fe22497"},"source":["def foo():\n","    print(\"starting...\")\n","    while True:\n","        res = yield 4\n","        print(\"res:\",res)\n","g = foo()\n","print(next(g))\n","print(\"*\"*20)\n","print(next(g))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["starting...\n","4\n","********************\n","res: None\n","4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sBR8Fpm6g1l-","executionInfo":{"status":"ok","timestamp":1617502933509,"user_tz":-480,"elapsed":1218,"user":{"displayName":"Ming Di","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhiYdtLRyWxEL5-a0628SLf-QrwCseeizEKVqj=s64","userId":"00101951631288157688"}},"outputId":"1ff4893c-bd91-4056-b657-92347ab215ee"},"source":["imgs, labels = train_set[0], train_set[1]\n","print(\"训练数据集数量: \", len(imgs))\n","# 获得数据集长度\n","imgs_length = len(imgs)\n","# 定义数据集每个数据的序号，根据序号读取数据\n","index_list = list(range(imgs_length))\n","# 读入数据时用到的批次大小\n","BATCHSIZE = 100\n","\n","# 随机打乱训练数据的索引序号\n","random.shuffle(index_list) \n","\n","# 定义数据生成器，返回批次数据\n","def data_generator():\n","\n","    imgs_list = []\n","    labels_list = []\n","    for i in index_list:\n","        # 将数据处理成希望的格式，比如类型为float32，shape为[1, 28, 28]\n","        img = np.reshape(imgs[i], [1, IMG_ROWS, IMG_COLS]).astype('float32')\n","        label = np.reshape(labels[i], [1]).astype('float32')\n","        imgs_list.append(img) \n","        labels_list.append(label)\n","        if len(imgs_list) == BATCHSIZE:\n","            # 获得一个batchsize的数据，并返回\n","            yield np.array(imgs_list), np.array(labels_list) #yield是python生成器\n","            # 清空数据读取列表\n","            imgs_list = []\n","            labels_list = []\n","\n","    # 如果剩余数据的数目小于BATCHSIZE，\n","    # 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch\n","    if len(imgs_list) > 0:\n","        yield np.array(imgs_list), np.array(labels_list)\n","    return data_generator"],"execution_count":20,"outputs":[{"output_type":"stream","text":["训练数据集数量:  50000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Syr1jZaUg1l_","executionInfo":{"status":"ok","timestamp":1617502935392,"user_tz":-480,"elapsed":945,"user":{"displayName":"Ming Di","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhiYdtLRyWxEL5-a0628SLf-QrwCseeizEKVqj=s64","userId":"00101951631288157688"}},"outputId":"40188b5d-822b-4a7c-cb7d-8b2e773eab36"},"source":["# 声明数据读取函数，从训练集中读取数据\n","train_loader = data_generator\n","# 以迭代的形式读取数据\n","for batch_id, data in enumerate(train_loader()):\n","    image_data, label_data = data\n","    if batch_id == 0:\n","        # 打印数据shape和类型\n","        print(\"打印第一个batch数据的维度:\")\n","        print(\"图像维度: {}, 标签维度: {}\".format(image_data.shape, label_data.shape))\n","    break"],"execution_count":21,"outputs":[{"output_type":"stream","text":["打印第一个batch数据的维度:\n","图像维度: (100, 1, 28, 28), 标签维度: (100, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"rUv7SJ53g1l_"},"source":["# 校验数据有效性\n","\n","在实际应用中，原始数据可能存在标注不准确、数据杂乱或格式不统一等情况。因此在完成数据处理流程后，还需要进行数据校验，一般有两种方式：\n","\n","* 机器校验：加入一些校验和清理数据的操作。\n","* 人工校验：先打印数据输出结果，观察是否是设置的格式。再从训练的结果验证数据处理和读取的有效性。\n","\n","##  机器校验\n","\n","如下代码所示，如果数据集中的图片数量和标签数量不等，说明数据逻辑存在问题，可使用assert语句校验图像数量和标签数据是否一致。"]},{"cell_type":"code","metadata":{"id":"EvtAORQOg1mA","executionInfo":{"status":"ok","timestamp":1617503390838,"user_tz":-480,"elapsed":1118,"user":{"displayName":"Ming Di","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhiYdtLRyWxEL5-a0628SLf-QrwCseeizEKVqj=s64","userId":"00101951631288157688"}}},"source":["imgs_length = len(imgs)\n","\n","assert len(imgs) == len(labels), \\\n","    \"length of train_imgs({}) should be the same as train_labels({})\".format(len(imgs), len(label))"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"g0MI8nAdg1mA"},"source":["## 人工校验\n","\n","人工校验是指打印数据输出结果，观察是否是预期的格式。实现数据处理和加载函数后，我们可以调用它读取一次数据，观察数据的shape和类型是否与函数中设置的一致。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xbWo6GBeg1mB","executionInfo":{"status":"ok","timestamp":1617501441199,"user_tz":-480,"elapsed":20098,"user":{"displayName":"Ming Di","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhiYdtLRyWxEL5-a0628SLf-QrwCseeizEKVqj=s64","userId":"00101951631288157688"}},"outputId":"f49030ca-21ff-4442-8053-c9f167a6cc98"},"source":["# 声明数据读取函数，从训练集中读取数据\n","train_loader = data_generator\n","# 以迭代的形式读取数据\n","for batch_id, data in enumerate(train_loader()):\n","    image_data, label_data = data\n","    if batch_id == 0:\n","        # 打印数据shape和类型\n","        print(\"打印第一个batch数据的维度，以及数据的类型:\")\n","        print(\"图像维度: {}, 标签维度: {}, 图像数据类型: {}, 标签数据类型: {}\".format(image_data.shape, label_data.shape, type(image_data), type(label_data)))\n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["打印第一个batch数据的维度，以及数据的类型:\n","图像维度: (100, 1, 28, 28), 标签维度: (100, 1), 图像数据类型: <class 'numpy.ndarray'>, 标签数据类型: <class 'numpy.ndarray'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"j5FYvFdSg1mB"},"source":["# 封装数据读取与处理函数\n","\n","上文，我们从读取数据、划分数据集、到打乱训练数据、构建数据读取器以及数据数据校验，完成了一整套一般性的数据处理流程，下面将这些步骤放在一个函数中实现，方便在神经网络训练时直接调用。"]},{"cell_type":"code","metadata":{"id":"ZecaUSHJg1mB","executionInfo":{"status":"ok","timestamp":1617503563776,"user_tz":-480,"elapsed":1878,"user":{"displayName":"Ming Di","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhiYdtLRyWxEL5-a0628SLf-QrwCseeizEKVqj=s64","userId":"00101951631288157688"}}},"source":["def load_data(mode='train'):\n","    datafile = '/content/drive/MyDrive/AI Studio/json/mnist.json.gz'\n","    print('loading mnist dataset from {} ......'.format(datafile))\n","    # 加载json数据文件\n","    data = json.load(gzip.open(datafile))\n","    print('mnist dataset load done')\n","   \n","    # 读取到的数据区分训练集，验证集，测试集\n","    train_set, val_set, eval_set = data\n","    if mode=='train':\n","        # 获得训练数据集\n","        imgs, labels = train_set[0], train_set[1]\n","    elif mode=='valid':\n","        # 获得验证数据集\n","        imgs, labels = val_set[0], val_set[1]\n","    elif mode=='eval':\n","        # 获得测试数据集\n","        imgs, labels = eval_set[0], eval_set[1]\n","    else:\n","        raise Exception(\"mode can only be one of ['train', 'valid', 'eval']\")\n","    # print(\"训练数据集数量: \", len(imgs))\n","    \n","    # 校验数据\n","    imgs_length = len(imgs)\n","\n","    assert len(imgs) == len(labels), \\\n","          \"length of train_imgs({}) should be the same as train_labels({})\".format(len(imgs), len(label))\n","    \n","    # 获得数据集长度\n","    imgs_length = len(imgs)\n","    \n","    # 定义数据集每个数据的序号，根据序号读取数据\n","    index_list = list(range(imgs_length))\n","    # 读入数据时用到的批次大小\n","    BATCHSIZE = 100\n","    \n","    # 定义数据生成器\n","    def data_generator():\n","        if mode == 'train':\n","            # 训练模式下打乱数据\n","            random.shuffle(index_list)\n","        imgs_list = []\n","        labels_list = []\n","        for i in index_list:\n","            # 将数据处理成希望的格式，比如类型为float32，shape为[1, 28, 28]\n","            img = np.reshape(imgs[i], [1, IMG_ROWS, IMG_COLS]).astype('float32')\n","            label = np.reshape(labels[i], [1]).astype('float32')\n","            imgs_list.append(img) \n","            labels_list.append(label)\n","            if len(imgs_list) == BATCHSIZE:\n","                # 获得一个batchsize的数据，并返回\n","                yield np.array(imgs_list), np.array(labels_list)\n","                # 清空数据读取列表\n","                imgs_list = []\n","                labels_list = []\n","    \n","        # 如果剩余数据的数目小于BATCHSIZE，\n","        # 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch\n","        if len(imgs_list) > 0:\n","            yield np.array(imgs_list), np.array(labels_list)\n","    return data_generator"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"m_A3S88Ug1mC"},"source":["下面定义一层神经网络，利用定义好的数据处理函数，完成神经网络的训练。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ysKSSVEfg1mD","executionInfo":{"status":"ok","timestamp":1617501486865,"user_tz":-480,"elapsed":65752,"user":{"displayName":"Ming Di","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhiYdtLRyWxEL5-a0628SLf-QrwCseeizEKVqj=s64","userId":"00101951631288157688"}},"outputId":"2bbced1d-f88d-42e7-ec77-d8557a3dad56"},"source":["#数据处理部分之后的代码，数据读取的部分调用Load_data函数\n","# 定义网络结构，同上一节所使用的网络结构\n","class MNIST(fluid.dygraph.Layer):\n","    def __init__(self):\n","        super(MNIST, self).__init__()\n","        self.fc = Linear(input_dim=784, output_dim=1, act=None)\n","\n","    def forward(self, inputs):\n","        inputs = fluid.layers.reshape(inputs, (-1, 784))\n","        outputs = self.fc(inputs)\n","        return outputs\n","\n","# 训练配置，并启动训练过程\n","with fluid.dygraph.guard():\n","    model = MNIST()\n","    model.train()\n","    #调用加载数据的函数\n","    train_loader = load_data('train')\n","    optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.001, parameter_list=model.parameters())\n","    EPOCH_NUM = 10\n","    for epoch_id in range(EPOCH_NUM):\n","        for batch_id, data in enumerate(train_loader()):\n","            #准备数据，变得更加简洁\n","            image_data, label_data = data\n","            image = fluid.dygraph.to_variable(image_data)\n","            label = fluid.dygraph.to_variable(label_data)\n","            \n","            #前向计算的过程\n","            predict = model(image)\n","            \n","            #计算损失，取一个批次样本损失的平均值\n","            loss = fluid.layers.square_error_cost(predict, label)\n","            avg_loss = fluid.layers.mean(loss)\n","            \n","            #每训练了200批次的数据，打印下当前Loss的情况\n","            if batch_id % 200 == 0:\n","                print(\"epoch: {}, batch: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\n","            \n","            #后向传播，更新参数的过程\n","            avg_loss.backward()\n","            optimizer.minimize(avg_loss)\n","            model.clear_gradients()\n","\n","    #保存模型参数\n","    fluid.save_dygraph(model.state_dict(), 'mnist')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading mnist dataset from /content/drive/MyDrive/AI Studio/json/mnist.json.gz ......\n","mnist dataset load done\n","训练数据集数量:  50000\n","epoch: 0, batch: 0, loss is: [21.238441]\n","epoch: 0, batch: 200, loss is: [3.3079748]\n","epoch: 0, batch: 400, loss is: [4.132261]\n","epoch: 1, batch: 0, loss is: [4.0946083]\n","epoch: 1, batch: 200, loss is: [3.2244995]\n","epoch: 1, batch: 400, loss is: [3.2575388]\n","epoch: 2, batch: 0, loss is: [3.433447]\n","epoch: 2, batch: 200, loss is: [2.9914527]\n","epoch: 2, batch: 400, loss is: [3.296512]\n","epoch: 3, batch: 0, loss is: [3.124172]\n","epoch: 3, batch: 200, loss is: [4.5402603]\n","epoch: 3, batch: 400, loss is: [3.1207495]\n","epoch: 4, batch: 0, loss is: [2.6082337]\n","epoch: 4, batch: 200, loss is: [3.6794007]\n","epoch: 4, batch: 400, loss is: [3.7289393]\n","epoch: 5, batch: 0, loss is: [3.7761207]\n","epoch: 5, batch: 200, loss is: [4.2145042]\n","epoch: 5, batch: 400, loss is: [3.2356415]\n","epoch: 6, batch: 0, loss is: [3.4550097]\n","epoch: 6, batch: 200, loss is: [4.1474767]\n","epoch: 6, batch: 400, loss is: [3.0162067]\n","epoch: 7, batch: 0, loss is: [3.785287]\n","epoch: 7, batch: 200, loss is: [3.7861755]\n","epoch: 7, batch: 400, loss is: [2.8202114]\n","epoch: 8, batch: 0, loss is: [3.3004074]\n","epoch: 8, batch: 200, loss is: [3.54618]\n","epoch: 8, batch: 400, loss is: [3.5545194]\n","epoch: 9, batch: 0, loss is: [2.4455879]\n","epoch: 9, batch: 200, loss is: [3.8061094]\n","epoch: 9, batch: 400, loss is: [3.5704064]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"tjN6S4wIg1mE"},"source":["\n","# 异步数据读取\n","\n","上面提到的数据读取采用的是同步数据读取方式。对于样本量较大、数据读取较慢的场景，建议采用异步数据读取方式。异步读取数据时，数据读取和模型训练并行执行，从而加快了数据读取速度，牺牲一小部分内存换取数据读取效率的提升，二者关系如 **图4** 所示。\n","\n","<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b6fb955fa1d84181b9a791fae716191ab87908d800af429ab1312ce0bd53e725\" width=\"600\" ></center>\n","<center><br>图4：同步数据读取和异步数据读取示意图</br></center>\n","<br></br>\n","\n","* **同步数据读取**：数据读取与模型训练串行。当模型需要数据时，才运行数据读取函数获得当前批次的数据。在读取数据期间，模型一直等待数据读取结束才进行训练，数据读取速度相对较慢。\n","* **异步数据读取**：数据读取和模型训练并行。读取到的数据不断的放入缓存区，无需等待模型训练就可以启动下一轮数据读取。当模型训练完一个批次后，不用等待数据读取过程，直接从缓存区获得下一批次数据进行训练，从而加快了数据读取速度。\n","* **异步队列**：数据读取和模型训练交互的仓库，二者均可以从仓库中读取数据，它的存在使得两者的工作节奏可以解耦。\n","\n","使用飞桨实现异步数据读取非常简单，如下所示。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yIRHZeN4g1mE","executionInfo":{"status":"ok","timestamp":1617501499207,"user_tz":-480,"elapsed":78086,"user":{"displayName":"Ming Di","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhiYdtLRyWxEL5-a0628SLf-QrwCseeizEKVqj=s64","userId":"00101951631288157688"}},"outputId":"724444d1-5be7-4600-8fa2-9845c211290a"},"source":["# 定义数据读取后存放的位置，CPU或者GPU，这里使用CPU\n","# place = fluid.CUDAPlace(0) 时，数据读到GPU上\n","place = fluid.CPUPlace()\n","with fluid.dygraph.guard(place):\n","    # 声明数据加载函数，使用训练模式\n","    train_loader = load_data(mode='train')\n","    # 定义DataLoader对象用于加载Python生成器产生的数据,capacity仓库大小为5，true同步进行\n","    data_loader = fluid.io.DataLoader.from_generator(capacity=5, return_list=True)\n","    # 设置数据生成器\n","    data_loader.set_batch_generator(train_loader, places=place)\n","    # 迭代的读取数据并打印数据的形状\n","    for i, data in enumerate(data_loader):\n","        image_data, label_data = data\n","        print(i, image_data.shape, label_data.shape)\n","        if i>=5:\n","            break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading mnist dataset from /content/drive/MyDrive/AI Studio/json/mnist.json.gz ......\n","mnist dataset load done\n","训练数据集数量:  50000\n","0 [100, 1, 28, 28] [100, 1]\n","1 [100, 1, 28, 28] [100, 1]\n","2 [100, 1, 28, 28] [100, 1]\n","3 [100, 1, 28, 28] [100, 1]\n","4 [100, 1, 28, 28] [100, 1]\n","5 [100, 1, 28, 28] [100, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"3Jk6L_ZWg1mF"},"source":["与同步数据读取相比，异步数据读取仅增加了三行代码，如下所示。\n","\n","```python\n","place = fluid.CPUPlace()\n","\n","# 设置读取的数据是放在CPU还是GPU上。\n","\n","data_loader = fluid.io.DataLoader.from_generator(capacity=5, return_list=True) \n","\n","# 创建一个DataLoader对象用于加载Python生成器产生的数据。数据会由Python线程预先读取，并异步送入一个队列中。\n","\n","data_loader.set_batch_generator(train_loader, place) \n","\n","# 用创建的DataLoader对象设置一个数据生成器set_batch_generator，输入的参数是一个Python数据生成器train_loader和服务器资源类型place（标明CPU还是GPU）\n","```\n","\n","fluid.io.DataLoader.from_generator参数名称、参数含义、默认值如下：\n","\n","* 参数含义如下：\n","\n","  - feed_list        仅在paddle静态图中使用，动态图中设置为None，本教程默认使用动态图的建模方式。\n","  - capacity        表示在DataLoader中维护的队列容量，如果读取数据的速度很快，建议设置为更大的值。\n","  - use_double_buffer   是一个布尔型的参数，设置为True时Dataloader会预先异步读取下一个batch的数据放到缓存区。\n","  - iterable          表示创建的Dataloader对象是否是可迭代的，一般设置为True。\n","  - return_list        在动态图模式下需要设置为True，静态图模式下设置为False。\n","\n","\n","异步数据读取并训练的完整案例代码如下所示。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmqCRfp_g1mG","executionInfo":{"status":"ok","timestamp":1617501529086,"user_tz":-480,"elapsed":107958,"user":{"displayName":"Ming Di","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhiYdtLRyWxEL5-a0628SLf-QrwCseeizEKVqj=s64","userId":"00101951631288157688"}},"outputId":"41ccbb78-558d-47fc-f1be-6edbdbb77bf6"},"source":["with fluid.dygraph.guard():\n","    model = MNIST()\n","    model.train()\n","    #调用加载数据的函数\n","    train_loader = load_data('train')\n","    # 创建异步数据读取器\n","    place = fluid.CPUPlace()\n","    data_loader = fluid.io.DataLoader.from_generator(capacity=5, return_list=True)\n","    data_loader.set_batch_generator(train_loader, places=place)\n","    \n","    optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.001, parameter_list=model.parameters())\n","    EPOCH_NUM = 3\n","    for epoch_id in range(EPOCH_NUM):\n","        for batch_id, data in enumerate(data_loader):\n","            image_data, label_data = data\n","            image = fluid.dygraph.to_variable(image_data)\n","            label = fluid.dygraph.to_variable(label_data)\n","            \n","            predict = model(image)\n","            \n","            loss = fluid.layers.square_error_cost(predict, label)\n","            avg_loss = fluid.layers.mean(loss)\n","            \n","            if batch_id % 200 == 0:\n","                print(\"epoch: {}, batch: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\n","            \n","            avg_loss.backward()\n","            optimizer.minimize(avg_loss)\n","            model.clear_gradients()\n","\n","    fluid.save_dygraph(model.state_dict(), 'mnist')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading mnist dataset from /content/drive/MyDrive/AI Studio/json/mnist.json.gz ......\n","mnist dataset load done\n","训练数据集数量:  50000\n","epoch: 0, batch: 0, loss is: [26.336496]\n","epoch: 0, batch: 200, loss is: [3.7474244]\n","epoch: 0, batch: 400, loss is: [3.5638983]\n","epoch: 1, batch: 0, loss is: [4.0640807]\n","epoch: 1, batch: 200, loss is: [3.3489835]\n","epoch: 1, batch: 400, loss is: [3.2626474]\n","epoch: 2, batch: 0, loss is: [3.6971445]\n","epoch: 2, batch: 200, loss is: [4.9212008]\n","epoch: 2, batch: 400, loss is: [3.484805]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"zBCWHX6Xg1mG"},"source":["从异步数据读取的训练结果来看，损失函数下降与同步数据读取训练结果一致。注意，异步读取数据只在数据量规模巨大时会带来显著的性能提升，对于多数场景采用同步数据读取的方式已经足够。"]}]}