{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 概述\n",
    "\n",
    "上一节我们研究了资源部署优化的方法，通过使用单GPU和分布式部署，提升模型训练的效率。本节我们依旧横向展开\"横纵式\"，如 **图1** 所示，探讨在手写数字识别任务中，为了保证模型的真实效果，在模型训练部分，对模型进行一些调试和优化的方法。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/0f94e16e03f34223903319eb50c8c09687d203f0e718491bbc7c4fc1b2a88585\" width=\"1000\" hegiht=\"\" ></center>\n",
    "<center>图1：“横纵式”教学法 — 训练过程</center>\n",
    "<br></br>\n",
    "\n",
    "训练过程优化思路主要有如下五个关键环节：\n",
    "\n",
    "**1. 计算分类准确率，观测模型训练效果。**\n",
    "\n",
    "交叉熵损失函数只能作为优化目标，无法直接准确衡量模型的训练效果。准确率可以直接衡量训练效果，但由于其离散性质，不适合做为损失函数优化神经网络。\n",
    "    \n",
    "**2. 检查模型训练过程，识别潜在问题。**\n",
    "\n",
    "如果模型的损失或者评估指标表现异常，通常需要打印模型每一层的输入和输出来定位问题，分析每一层的内容来获取错误的原因。\n",
    "    \n",
    "**3. 加入校验或测试，更好评价模型效果。**\n",
    "\n",
    "理想的模型训练结果是在训练集和验证集上均有较高的准确率，如果训练集上的准确率高于验证集，说明网络训练程度不够；如果验证集的准确率高于训练集，可能是发生了过拟合现象。通过在优化目标中加入正则化项的办法，解决过拟合的问题。\n",
    "    \n",
    "**4. 加入正则化项，避免模型过拟合。**\n",
    "\n",
    "飞桨框架支持为整体参数加入正则化项，这是通常的做法。此外，飞桨框架也支持为某一层或某一部分的网络单独加入正则化项，以达到精细调整参数训练的效果。\n",
    "\n",
    "**5. 可视化分析。**\n",
    "\n",
    "用户不仅可以通过打印或使用matplotlib库作图，飞桨还提供了更专业的可视化分析工具VisualDL，提供便捷的可视化分析方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 计算模型的分类准确率\n",
    "\n",
    "准确率是一个直观衡量分类模型效果的指标，由于这个指标是离散的，因此不适合作为损失来优化。通常情况下，交叉熵损失越小的模型，分类的准确率也越高。基于分类准确率，我们可以公平的比较两种损失函数的优劣，例如[【手写数字识别】之损失函数 ](https://aistudio.baidu.com/aistudio/projectdetail/575664)章节中均方误差和交叉熵的比较。\n",
    "\n",
    "飞桨提供了计算分类准确率的API，使用[fluid.layers.accuracy](https://www.paddlepaddle.org.cn/documentation/docs/zh/api_cn/layers_cn/accuracy_cn.html#accuracy)可以直接计算准确率，该API的输入参数input为预测的分类结果predict，输入参数label为数据真实的label。\n",
    "\n",
    "在下述代码中，我们在模型前向计算过程forward函数中计算分类准确率，并在训练时打印每个批次样本的分类准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 加载相关库\n",
    "import os\n",
    "import random\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "from paddle.fluid.dygraph.nn import Conv2D, Pool2D, Linear\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "# 定义数据集读取器\n",
    "def load_data(mode='train'):\n",
    "\n",
    "    # 读取数据文件\n",
    "    datafile = './work/mnist.json.gz'\n",
    "    print('loading mnist dataset from {} ......'.format(datafile))\n",
    "    data = json.load(gzip.open(datafile))\n",
    "    # 读取数据集中的训练集，验证集和测试集\n",
    "    train_set, val_set, eval_set = data\n",
    "\n",
    "    # 数据集相关参数，图片高度IMG_ROWS, 图片宽度IMG_COLS\n",
    "    IMG_ROWS = 28\n",
    "    IMG_COLS = 28\n",
    "    # 根据输入mode参数决定使用训练集，验证集还是测试\n",
    "    if mode == 'train':\n",
    "        imgs = train_set[0]\n",
    "        labels = train_set[1]\n",
    "    elif mode == 'valid':\n",
    "        imgs = val_set[0]\n",
    "        labels = val_set[1]\n",
    "    elif mode == 'eval':\n",
    "        imgs = eval_set[0]\n",
    "        labels = eval_set[1]\n",
    "    # 获得所有图像的数量\n",
    "    imgs_length = len(imgs)\n",
    "    # 验证图像数量和标签数量是否一致\n",
    "    assert len(imgs) == len(labels), \\\n",
    "          \"length of train_imgs({}) should be the same as train_labels({})\".format(\n",
    "                  len(imgs), len(labels))\n",
    "\n",
    "    index_list = list(range(imgs_length))\n",
    "\n",
    "    # 读入数据时用到的batchsize\n",
    "    BATCHSIZE = 100\n",
    "\n",
    "    # 定义数据生成器\n",
    "    def data_generator():\n",
    "        # 训练模式下，打乱训练数据\n",
    "        if mode == 'train':\n",
    "            random.shuffle(index_list)\n",
    "        imgs_list = []\n",
    "        labels_list = []\n",
    "        # 按照索引读取数据\n",
    "        for i in index_list:\n",
    "            # 读取图像和标签，转换其尺寸和类型\n",
    "            img = np.reshape(imgs[i], [1, IMG_ROWS, IMG_COLS]).astype('float32')\n",
    "            label = np.reshape(labels[i], [1]).astype('int64')\n",
    "            imgs_list.append(img) \n",
    "            labels_list.append(label)\n",
    "            # 如果当前数据缓存达到了batch size，就返回一个批次数据\n",
    "            if len(imgs_list) == BATCHSIZE:\n",
    "                yield np.array(imgs_list), np.array(labels_list)\n",
    "                # 清空数据缓存列表\n",
    "                imgs_list = []\n",
    "                labels_list = []\n",
    "\n",
    "        # 如果剩余数据的数目小于BATCHSIZE，\n",
    "        # 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch\n",
    "        if len(imgs_list) > 0:\n",
    "            yield np.array(imgs_list), np.array(labels_list)\n",
    "\n",
    "    return data_generator\n",
    "\n",
    "\n",
    "# 定义模型结构\n",
    "class MNIST(fluid.dygraph.Layer):\n",
    "     def __init__(self):\n",
    "         super(MNIST, self).__init__()\n",
    "         \n",
    "         # 定义一个卷积层，使用relu激活函数\n",
    "         self.conv1 = Conv2D(num_channels=1, num_filters=20, filter_size=5, stride=1, padding=2, act='relu')\n",
    "         # 定义一个池化层，池化核为2，步长为2，使用最大池化方式\n",
    "         self.pool1 = Pool2D(pool_size=2, pool_stride=2, pool_type='max')\n",
    "         # 定义一个卷积层，使用relu激活函数\n",
    "         self.conv2 = Conv2D(num_channels=20, num_filters=20, filter_size=5, stride=1, padding=2, act='relu')\n",
    "         # 定义一个池化层，池化核为2，步长为2，使用最大池化方式\n",
    "         self.pool2 = Pool2D(pool_size=2, pool_stride=2, pool_type='max')\n",
    "         # 定义一个全连接层，输出节点数为10 \n",
    "         self.fc = Linear(input_dim=980, output_dim=10, act='softmax')\n",
    "    # 定义网络的前向计算过程\n",
    "     def forward(self, inputs, label):\n",
    "         x = self.conv1(inputs)\n",
    "         x = self.pool1(x)\n",
    "         x = self.conv2(x)\n",
    "         x = self.pool2(x)\n",
    "         x = fluid.layers.reshape(x, [x.shape[0], 980])\n",
    "         x = self.fc(x)\n",
    "         if label is not None:\n",
    "             acc = fluid.layers.accuracy(input=x, label=label)\n",
    "             return x, acc\n",
    "         else:\n",
    "             return x\n",
    "\n",
    "#调用加载数据的函数\n",
    "train_loader = load_data('train')\n",
    "    \n",
    "#在使用GPU机器时，可以将use_gpu变量设置成True\n",
    "use_gpu = False\n",
    "place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "\n",
    "with fluid.dygraph.guard(place):\n",
    "    model = MNIST()\n",
    "    model.train() \n",
    "    \n",
    "    #四种优化算法的设置方案，可以逐一尝试效果\n",
    "    optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.01, parameter_list=model.parameters())\n",
    "    #optimizer = fluid.optimizer.MomentumOptimizer(learning_rate=0.01, momentum=0.9, parameter_list=model.parameters())\n",
    "    #optimizer = fluid.optimizer.AdagradOptimizer(learning_rate=0.01, parameter_list=model.parameters())\n",
    "    #optimizer = fluid.optimizer.AdamOptimizer(learning_rate=0.01, parameter_list=model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 5\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            #准备数据\n",
    "            image_data, label_data = data\n",
    "            image = fluid.dygraph.to_variable(image_data)\n",
    "            label = fluid.dygraph.to_variable(label_data)\n",
    "            \n",
    "            #前向计算的过程，同时拿到模型输出值和分类准确率\n",
    "            predict, acc = model(image, label)\n",
    "            \n",
    "            #计算损失，取一个批次样本损失的平均值\n",
    "            loss = fluid.layers.cross_entropy(predict, label)\n",
    "            avg_loss = fluid.layers.mean(loss)\n",
    "            \n",
    "            #每训练了200批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 200 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "            \n",
    "            #后向传播，更新参数的过程\n",
    "            avg_loss.backward()\n",
    "            optimizer.minimize(avg_loss)\n",
    "            model.clear_gradients()\n",
    "\n",
    "    #保存模型参数\n",
    "    fluid.save_dygraph(model.state_dict(), 'mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 检查模型训练过程，识别潜在训练问题\n",
    "\n",
    "使用飞桨动态图编程可以方便的查看和调试训练的执行过程。在网络定义的Forward函数中，可以打印每一层输入输出的尺寸，以及每层网络的参数。通过查看这些信息，不仅可以更好地理解训练的执行过程，还可以发现潜在问题，或者启发继续优化的思路。\n",
    "\n",
    "在下述程序中，使用``check_shape``变量控制是否打印“尺寸”，验证网络结构是否正确。使用``check_content``变量控制是否打印“内容值”，验证数据分布是否合理。假如在训练中发现中间层的部分输出持续为0，说明该部分的网络结构设计存在问题，没有充分利用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义模型结构\n",
    "class MNIST(fluid.dygraph.Layer):\n",
    "     def __init__(self):\n",
    "         super(MNIST, self).__init__()\n",
    "         \n",
    "         # 定义一个卷积层，使用relu激活函数\n",
    "         self.conv1 = Conv2D(num_channels=1, num_filters=20, filter_size=5, stride=1, padding=2, act='relu')\n",
    "         # 定义一个池化层，池化核为2，步长为2，使用最大池化方式\n",
    "         self.pool1 = Pool2D(pool_size=2, pool_stride=2, pool_type='max')\n",
    "         # 定义一个卷积层，使用relu激活函数\n",
    "         self.conv2 = Conv2D(num_channels=20, num_filters=20, filter_size=5, stride=1, padding=2, act='relu')\n",
    "         # 定义一个池化层，池化核为2，步长为2，使用最大池化方式\n",
    "         self.pool2 = Pool2D(pool_size=2, pool_stride=2, pool_type='max')\n",
    "         # 定义一个全连接层，输出节点数为10 \n",
    "         self.fc = Linear(input_dim=980, output_dim=10, act='softmax')\n",
    "     \n",
    "     #加入对每一层输入和输出的尺寸和数据内容的打印，根据check参数决策是否打印每层的参数和输出尺寸\n",
    "     def forward(self, inputs, label=None, check_shape=False, check_content=False):\n",
    "         # 给不同层的输出不同命名，方便调试\n",
    "         outputs1 = self.conv1(inputs)\n",
    "         outputs2 = self.pool1(outputs1)\n",
    "         outputs3 = self.conv2(outputs2)\n",
    "         outputs4 = self.pool2(outputs3)\n",
    "         _outputs4 = fluid.layers.reshape(outputs4, [outputs4.shape[0], -1])\n",
    "         outputs5 = self.fc(_outputs4)\n",
    "         \n",
    "         # 选择是否打印神经网络每层的参数尺寸和输出尺寸，验证网络结构是否设置正确\n",
    "         if check_shape:\n",
    "             # 打印每层网络设置的超参数-卷积核尺寸，卷积步长，卷积padding，池化核尺寸\n",
    "             print(\"\\n########## print network layer's superparams ##############\")\n",
    "             print(\"conv1-- kernel_size:{}, padding:{}, stride:{}\".format(self.conv1.weight.shape, self.conv1._padding, self.conv1._stride))\n",
    "             print(\"conv2-- kernel_size:{}, padding:{}, stride:{}\".format(self.conv2.weight.shape, self.conv2._padding, self.conv2._stride))\n",
    "             print(\"pool1-- pool_type:{}, pool_size:{}, pool_stride:{}\".format(self.pool1._pool_type, self.pool1._pool_size, self.pool1._pool_stride))\n",
    "             print(\"pool2-- pool_type:{}, poo2_size:{}, pool_stride:{}\".format(self.pool2._pool_type, self.pool2._pool_size, self.pool2._pool_stride))\n",
    "             print(\"fc-- weight_size:{}, bias_size_{}, activation:{}\".format(self.fc.weight.shape, self.fc.bias.shape, self.fc._act))\n",
    "             \n",
    "             # 打印每层的输出尺寸\n",
    "             print(\"\\n########## print shape of features of every layer ###############\")\n",
    "             print(\"inputs_shape: {}\".format(inputs.shape))\n",
    "             print(\"outputs1_shape: {}\".format(outputs1.shape))\n",
    "             print(\"outputs2_shape: {}\".format(outputs2.shape))\n",
    "             print(\"outputs3_shape: {}\".format(outputs3.shape))\n",
    "             print(\"outputs4_shape: {}\".format(outputs4.shape))\n",
    "             print(\"outputs5_shape: {}\".format(outputs5.shape))\n",
    "             \n",
    "         # 选择是否打印训练过程中的参数和输出内容，可用于训练过程中的调试\n",
    "         if check_content:\n",
    "            # 打印卷积层的参数-卷积核权重，权重参数较多，此处只打印部分参数\n",
    "             print(\"\\n########## print convolution layer's kernel ###############\")\n",
    "             print(\"conv1 params -- kernel weights:\", self.conv1.weight[0][0])\n",
    "             print(\"conv2 params -- kernel weights:\", self.conv2.weight[0][0])\n",
    "\n",
    "             # 创建随机数，随机打印某一个通道的输出值\n",
    "             idx1 = np.random.randint(0, outputs1.shape[1])\n",
    "             idx2 = np.random.randint(0, outputs3.shape[1])\n",
    "             # 打印卷积-池化后的结果，仅打印batch中第一个图像对应的特征\n",
    "             print(\"\\nThe {}th channel of conv1 layer: \".format(idx1), outputs1[0][idx1])\n",
    "             print(\"The {}th channel of conv2 layer: \".format(idx2), outputs3[0][idx2])\n",
    "             print(\"The output of last layer:\", outputs5[0], '\\n')\n",
    "            \n",
    "        # 如果label不是None，则计算分类精度并返回\n",
    "         if label is not None:\n",
    "             acc = fluid.layers.accuracy(input=outputs5, label=label)\n",
    "             return outputs5, acc\n",
    "         else:\n",
    "             return outputs5\n",
    "\n",
    "    \n",
    "#在使用GPU机器时，可以将use_gpu变量设置成True\n",
    "use_gpu = False\n",
    "place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "\n",
    "with fluid.dygraph.guard(place):\n",
    "    model = MNIST()\n",
    "    model.train() \n",
    "    \n",
    "    #四种优化算法的设置方案，可以逐一尝试效果\n",
    "    optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.01, parameter_list=model.parameters())\n",
    "    #optimizer = fluid.optimizer.MomentumOptimizer(learning_rate=0.01, momentum=0.9, parameter_list=model.parameters())\n",
    "    #optimizer = fluid.optimizer.AdagradOptimizer(learning_rate=0.01, parameter_list=model.parameters())\n",
    "    #optimizer = fluid.optimizer.AdamOptimizer(learning_rate=0.01, parameter_list=model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 1\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            #准备数据，变得更加简洁\n",
    "            image_data, label_data = data\n",
    "            image = fluid.dygraph.to_variable(image_data)\n",
    "            label = fluid.dygraph.to_variable(label_data)\n",
    "            \n",
    "            #前向计算的过程，同时拿到模型输出值和分类准确率\n",
    "            if batch_id == 0 and epoch_id==0:\n",
    "                # 打印模型参数和每层输出的尺寸\n",
    "                predict, acc = model(image, label, check_shape=True, check_content=False)\n",
    "            elif batch_id==401:\n",
    "                # 打印模型参数和每层输出的值\n",
    "                predict, acc = model(image, label, check_shape=False, check_content=True)\n",
    "            else:\n",
    "                predict, acc = model(image, label)\n",
    "            \n",
    "            #计算损失，取一个批次样本损失的平均值\n",
    "            loss = fluid.layers.cross_entropy(predict, label)\n",
    "            avg_loss = fluid.layers.mean(loss)\n",
    "            \n",
    "            #每训练了100批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 200 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "            \n",
    "            #后向传播，更新参数的过程\n",
    "            avg_loss.backward()\n",
    "            optimizer.minimize(avg_loss)\n",
    "            model.clear_gradients()\n",
    "\n",
    "    #保存模型参数\n",
    "    fluid.save_dygraph(model.state_dict(), 'mnist')\n",
    "    print(\"Model has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入校验或测试，更好评价模型效果 \n",
    "\n",
    "在训练过程中，我们会发现模型在训练样本集上的损失在不断减小。但这是否代表模型在未来的应用场景上依然有效？为了验证模型的有效性，通常将样本集合分成三份，训练集、校验集和测试集。\n",
    "\n",
    "- **训练集** ：用于训练模型的参数，即训练过程中主要完成的工作。\n",
    "- **校验集** ：用于对模型超参数的选择，比如网络结构的调整、正则化项权重的选择等。\n",
    "- **测试集** ：用于模拟模型在应用后的真实效果。因为测试集没有参与任何模型优化或参数训练的工作，所以它对模型来说是完全未知的样本。在不以校验数据优化网络结构或模型超参数时，校验数据和测试数据的效果是类似的，均更真实的反映模型效果。\n",
    "\n",
    "如下程序读取上一步训练保存的模型参数，读取测试数据集，并测试模型在测试数据集上的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with fluid.dygraph.guard():\n",
    "    print('start evaluation .......')\n",
    "    #加载模型参数\n",
    "    model = MNIST()\n",
    "    model_state_dict, _ = fluid.load_dygraph('mnist')\n",
    "    model.load_dict(model_state_dict)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loader = load_data('eval')\n",
    "\n",
    "    acc_set = []\n",
    "    avg_loss_set = []\n",
    "    for batch_id, data in enumerate(eval_loader()):\n",
    "        x_data, y_data = data\n",
    "        img = fluid.dygraph.to_variable(x_data)\n",
    "        label = fluid.dygraph.to_variable(y_data)\n",
    "        prediction, acc = model(img, label)\n",
    "        loss = fluid.layers.cross_entropy(input=prediction, label=label)\n",
    "        avg_loss = fluid.layers.mean(loss)\n",
    "        acc_set.append(float(acc.numpy()))\n",
    "        avg_loss_set.append(float(avg_loss.numpy()))\n",
    "    \n",
    "    #计算多个batch的平均损失和准确率\n",
    "    acc_val_mean = np.array(acc_set).mean()\n",
    "    avg_loss_val_mean = np.array(avg_loss_set).mean()\n",
    "\n",
    "    print('loss={}, acc={}'.format(avg_loss_val_mean, acc_val_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从测试的效果来看，模型在验证集上依然有93%的准确率，证明它是有预测效果的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入正则化项，避免模型过拟合\n",
    "   \n",
    "## 过拟合现象\n",
    "\n",
    "对于样本量有限、但需要使用强大模型的复杂任务，模型很容易出现过拟合的表现，即在训练集上的损失小，在验证集或测试集上的损失较大，如 **图2** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/99b879c21113494a9d7315eeda74bc4c8fea07f984824a03bf8411e946c75f1b\" width=\"400\" hegiht=\"\" ></center>\n",
    "<center><br>图2：过拟合现象，训练误差不断降低，但测试误差先降后增</br></center>\n",
    "<br></br>\n",
    "\n",
    "反之，如果模型在训练集和测试集上均损失较大，则称为欠拟合。过拟合表示模型过于敏感，学习到了训练数据中的一些误差，而这些误差并不是真实的泛化规律（可推广到测试集上的规律）。欠拟合表示模型还不够强大，还没有很好的拟合已知的训练样本，更别提测试样本了。因为欠拟合情况容易观察和解决，只要训练loss不够好，就不断使用更强大的模型即可，因此实际中我们更需要处理好过拟合的问题。\n",
    "\n",
    "## 导致过拟合原因\n",
    "\n",
    "造成过拟合的原因是模型过于敏感，而训练数据量太少或其中的噪音太多。\n",
    "\n",
    "如**图3** 所示，理想的回归模型是一条坡度较缓的抛物线，欠拟合的模型只拟合出一条直线，显然没有捕捉到真实的规律，但过拟合的模型拟合出存在很多拐点的抛物线，显然是过于敏感，也没有正确表达真实规律。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/53c389bb3c824706bd2fbc05f83ab0c6dd6b5b2fdedb4150a17e16a1b64c243e\" width=\"700\" hegiht=\"\" ></center>\n",
    "<center><br>图3：回归模型的过拟合，理想和欠拟合状态的表现</br></center>\n",
    "<br></br>\n",
    "\n",
    "如**图4** 所示，理想的分类模型是一条半圆形的曲线，欠拟合用直线作为分类边界，显然没有捕捉到真实的边界，但过拟合的模型拟合出很扭曲的分类边界，虽然对所有的训练数据正确分类，但对一些较为个例的样本所做出的妥协，高概率不是真实的规律。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/ea1c9072d5d54f1d85af807a7fbaf115772892e3228e4fc895cf2c660b190d62\" width=\"700\" hegiht=\"\" ></center>\n",
    "<center><br>图4：分类模型的欠拟合，理想和过拟合状态的表现</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 过拟合的成因与防控\n",
    "\n",
    "为了更好的理解过拟合的成因，可以参考侦探定位罪犯的案例逻辑，如 **图5** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/34de60a675b64468a2c3fee0844a168d53e891eaacf643fd8c1c9ba8e3812bcc\" width=\"700\" hegiht=\"\" ></center>\n",
    "<center><br>图5：侦探定位罪犯与模型假设示意</br></center>\n",
    "<br></br>\n",
    "\n",
    "**对于这个案例，假设侦探也会犯错，通过分析发现可能的原因：**\n",
    "\n",
    "1. 情况1：罪犯证据存在错误，依据错误的证据寻找罪犯肯定是缘木求鱼。\n",
    "\n",
    "2. 情况2：搜索范围太大的同时证据太少，导致符合条件的候选（嫌疑人）太多，无法准确定位罪犯。\n",
    "\n",
    "**那么侦探解决这个问题的方法有两种：或者缩小搜索范围（比如假设该案件只能是熟人作案），或者寻找更多的证据。**\n",
    "\n",
    "**归结到深度学习中，假设模型也会犯错，通过分析发现可能的原因：**\n",
    "\n",
    "1. 情况1：训练数据存在噪音，导致模型学到了噪音，而不是真实规律。\n",
    "\n",
    "2. 情况2：使用强大模型（表示空间大）的同时训练数据太少，导致在训练数据上表现良好的候选假设太多，锁定了一个“虚假正确”的假设。\n",
    "\n",
    "**对于情况1，我们使用数据清洗和修正来解决。 对于情况2，我们或者限制模型表示能力，或者收集更多的训练数据。**\n",
    "\n",
    "而清洗训练数据中的错误，或收集更多的训练数据往往是一句“正确的废话”，在任何时候我们都想获得更多更高质量的数据。在实际项目中，更快、更低成本可控制过拟合的方法，只有限制模型的表示能力。\n",
    "\n",
    "## 正则化项\n",
    "\n",
    "为了防止模型过拟合，在没有扩充样本量的可能下，只能降低模型的复杂度，可以通过限制参数的数量或可能取值（参数值尽量小）实现。\n",
    "\n",
    "具体来说，在模型的优化目标（损失）中人为加入对参数规模的惩罚项。当参数越多或取值越大时，该惩罚项就越大。通过调整惩罚项的权重系数，可以使模型在“尽量减少训练损失”和“保持模型的泛化能力”之间取得平衡。泛化能力表示模型在没有见过的样本上依然有效。正则化项的存在，增加了模型在训练集上的损失。\n",
    "\n",
    "飞桨支持为所有参数加上统一的正则化项，也支持为特定的参数添加正则化项。前者的实现如下代码所示，仅在优化器中设置``regularization``参数即可实现。使用参数``regularization_coeff``调节正则化项的权重，权重越大时，对模型复杂度的惩罚越高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with fluid.dygraph.guard():\n",
    "    model = MNIST()\n",
    "    model.train() \n",
    "    \n",
    "    #各种优化算法均可以加入正则化项，避免过拟合，参数regularization_coeff调节正则化项的权重\n",
    "    #optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.01, regularization=fluid.regularizer.L2Decay(regularization_coeff=0.1),parameter_list=model.parameters()))\n",
    "    optimizer = fluid.optimizer.AdamOptimizer(learning_rate=0.01, regularization=fluid.regularizer.L2Decay(regularization_coeff=0.1),\n",
    "                                              parameter_list=model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 10\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            #准备数据，变得更加简洁\n",
    "            image_data, label_data = data\n",
    "            image = fluid.dygraph.to_variable(image_data)\n",
    "            label = fluid.dygraph.to_variable(label_data)\n",
    "            \n",
    "            #前向计算的过程，同时拿到模型输出值和分类准确率\n",
    "            predict, acc = model(image, label)\n",
    "\n",
    "            #计算损失，取一个批次样本损失的平均值\n",
    "            loss = fluid.layers.cross_entropy(predict, label)\n",
    "            avg_loss = fluid.layers.mean(loss)\n",
    "            \n",
    "            #每训练了100批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "            \n",
    "            #后向传播，更新参数的过程\n",
    "            avg_loss.backward()\n",
    "            optimizer.minimize(avg_loss)\n",
    "            model.clear_gradients()\n",
    "\n",
    "    #保存模型参数\n",
    "    fluid.save_dygraph(model.state_dict(), 'mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 可视化分析\n",
    "\n",
    "训练模型时，经常需要观察模型的评价指标，分析模型的优化过程，以确保训练是有效的。可选用这两种工具：Matplotlib库和VisualDL。\n",
    "\n",
    "* **Matplotlib库**：Matplotlib库是Python中使用的最多的2D图形绘图库，它有一套完全仿照MATLAB的函数形式的绘图接口，使用轻量级的PLT库（Matplotlib）作图是非常简单的。\n",
    "* **VisualDL**：如果期望使用更加专业的作图工具，可以尝试VisualDL。VisualDL能够有效地展示飞桨框架在运行过程中的计算图、各种指标随着时间的变化趋势以及训练中使用到的数据信息。\n",
    "\n",
    "## 使用Matplotlib库绘制损失随训练下降的曲线图\n",
    "\n",
    "将训练的批次编号作为X轴坐标，该批次的训练损失作为Y轴坐标。\n",
    "\n",
    "1. 训练开始前，声明两个列表变量存储对应的批次编号(iters=[])和训练损失(losses=[])。\n",
    "\n",
    "2. 随着训练的进行，将iter和losses两个列表填满。\n",
    "\n",
    "3. 训练结束后，将两份数据以参数形式导入PLT的横纵坐标。\n",
    "```\n",
    "plt.xlabel(\"iter\", fontsize=14)，plt.ylabel(\"loss\", fontsize=14)\n",
    "```\n",
    "4. 最后，调用plt.plot()函数即可完成作图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#引入matplotlib库\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with fluid.dygraph.guard(place):\n",
    "    model = MNIST()\n",
    "    model.train() \n",
    "    \n",
    "    optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.01, parameter_list=model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 10\n",
    "    iter=0\n",
    "    iters=[]\n",
    "    losses=[]\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            #准备数据，变得更加简洁\n",
    "            image_data, label_data = data\n",
    "            image = fluid.dygraph.to_variable(image_data)\n",
    "            label = fluid.dygraph.to_variable(label_data)\n",
    "            \n",
    "            #前向计算的过程，同时拿到模型输出值和分类准确率\n",
    "            predict, acc = model(image, label)\n",
    "\n",
    "            #计算损失，取一个批次样本损失的平均值\n",
    "            loss = fluid.layers.cross_entropy(predict, label)\n",
    "            avg_loss = fluid.layers.mean(loss)\n",
    "            \n",
    "            #每训练了100批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                iters.append(iter)\n",
    "                losses.append(avg_loss.numpy())\n",
    "                iter = iter + 100\n",
    "\n",
    "            #后向传播，更新参数的过程\n",
    "            avg_loss.backward()\n",
    "            optimizer.minimize(avg_loss)\n",
    "            model.clear_gradients()\n",
    "\n",
    "    #保存模型参数\n",
    "    fluid.save_dygraph(model.state_dict(), 'mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#画出训练过程中Loss的变化曲线\n",
    "plt.figure()\n",
    "plt.title(\"train loss\", fontsize=24)\n",
    "plt.xlabel(\"iter\", fontsize=14)\n",
    "plt.ylabel(\"loss\", fontsize=14)\n",
    "plt.plot(iters, losses,color='red',label='train loss') \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 使用VisualDL可视化分析\n",
    "\n",
    "VisualDL由第三方生态研发集成进飞桨，它的使用不复杂，可分为四个步骤。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "本案例不支持AI studio演示，请读者在本地安装的飞桨上实践。\n",
    "\n",
    "------\n",
    "\n",
    "* 步骤1：引入VisualDL库，定义作图数据存储位置（供第3步使用），本案例的路径是“log”。\n",
    "```\n",
    "from visualdl import LogWriter\n",
    "log_writer = LogWriter(\"./log\")\n",
    "```\n",
    "* 步骤2：在训练过程中插入作图语句。当每100个batch训练完成后，将当前损失作为一个新增的数据点(iter和acc的映射对)存储到第一步设置的文件中。使用变量iter记录下已经训练的批次数，作为作图的X轴坐标。\n",
    "\n",
    "```\n",
    "log_writer.add_scalar(tag = 'acc', step = iter, value = avg_acc.numpy())\n",
    "log_writer.add_scalar(tag = 'loss', step = iter, value = avg_loss.numpy())\n",
    "iter = iter + 100\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting visualdl\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/1e/1e/d92ce71705b0de5e5fde210fc7369ee9a1aa6a53065a83c968b655885b9a/visualdl-2.0.0b6-py3-none-any.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 154kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl) (4.1.1.26)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl) (1.1.1)\n",
      "Collecting hdfs (from visualdl)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/82/39/2c0879b1bcfd1f6ad078eb210d09dbce21072386a3997074ee91e60ddc5a/hdfs-2.5.8.tar.gz (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 29.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl) (2.22.0)\n",
      "Collecting Flask-Babel>=1.0.0 (from visualdl)\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='mirrors.tuna.tsinghua.edu.cn', port=443): Read timed out. (read timeout=15)\")': /pypi/web/simple/flask-babel/\u001b[0m\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/76/a4/0115c7c520125853037fc1d6b3da132a526949640e27a699a13e05ec7593/Flask_Babel-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl) (3.10.0)\n",
      "Collecting Pillow>=7.0.0 (from visualdl)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/ab/f8/d3627cc230270a6a4eedee32974fbc8cb26c5fdb8710dd5ea70133640022/Pillow-7.1.2-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 150kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.14.0 (from visualdl)\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.mirrors.ustc.edu.cn', port=443): Read timed out. (read timeout=15)\")': /simple/six/\u001b[0m\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl) (3.7.9)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl) (2.10.3)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl) (0.16.0)\n",
      "Collecting docopt (from hdfs->visualdl)\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.mirrors.ustc.edu.cn', port=443): Read timed out. (read timeout=15)\")': /simple/docopt/\u001b[0m\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl) (2.8)\n",
      "Collecting Babel>=2.3 (from Flask-Babel>=1.0.0->visualdl)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/15/a1/522dccd23e5d2e47aed4b6a16795b8213e3272c7506e625f2425ad025a19/Babel-2.8.0-py2.py3-none-any.whl (8.6MB)\n",
      "\u001b[K     |████████████████████████████████| 8.6MB 219kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from protobuf>=3.1.0->visualdl) (41.4.0)\n",
      "Requirement already satisfied, skipping upgrade: entrypoints<0.4.0,>=0.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl) (0.3)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.6.0,>=2.5.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.2.0,>=2.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl) (7.2.0)\n",
      "Building wheels for collected packages: hdfs, docopt\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.5.8-cp37-none-any.whl size=33214 sha256=e4ef2c169683d0659c59316358a8b6ec9a204b93d96d9e67f8b0fcf05e8bfff5\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/35/91/05/ed325f80520cc72b4eaa7327f96358c62d84afd098625ed2bd\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=8bdeef3a9071e2a90c87577ad4a1a06bd5e8440e4286f5bab04535056d8f183b\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/78/da/5a/be54433e626178926da00dbc53e06294ba87ec2c37dded83b4\n",
      "Successfully built hdfs docopt\n",
      "Installing collected packages: docopt, six, hdfs, Babel, Flask-Babel, Pillow, visualdl\n",
      "  Found existing installation: six 1.12.0\n",
      "    Uninstalling six-1.12.0:\n",
      "      Successfully uninstalled six-1.12.0\n",
      "  Found existing installation: Pillow 6.2.0\n",
      "    Uninstalling Pillow-6.2.0:\n",
      "      Successfully uninstalled Pillow-6.2.0\n",
      "  Found existing installation: visualdl 1.3.0\n",
      "    Uninstalling visualdl-1.3.0:\n",
      "      Successfully uninstalled visualdl-1.3.0\n",
      "Successfully installed Babel-2.8.0 Flask-Babel-1.0.0 Pillow-7.1.2 docopt-0.6.2 hdfs-2.5.8 six-1.15.0 visualdl-2.0.0b6\n"
     ]
    }
   ],
   "source": [
    "# 安装visualdl\r\n",
    "!pip install --upgrade --pre visualdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mnist dataset from ./work/mnist.json.gz ......\n",
      "epoch: 0, batch: 0, loss is: [2.5931175], acc is [0.07]\n",
      "epoch: 0, batch: 100, loss is: [0.8559423], acc is [0.72]\n",
      "epoch: 0, batch: 200, loss is: [0.34354392], acc is [0.92]\n",
      "epoch: 0, batch: 300, loss is: [0.3946165], acc is [0.9]\n",
      "epoch: 0, batch: 400, loss is: [0.2425189], acc is [0.93]\n",
      "epoch: 1, batch: 0, loss is: [0.37391672], acc is [0.91]\n",
      "epoch: 1, batch: 100, loss is: [0.24383251], acc is [0.94]\n",
      "epoch: 1, batch: 200, loss is: [0.22635305], acc is [0.95]\n",
      "epoch: 1, batch: 300, loss is: [0.2740632], acc is [0.91]\n",
      "epoch: 1, batch: 400, loss is: [0.21924382], acc is [0.93]\n",
      "epoch: 2, batch: 0, loss is: [0.08771554], acc is [0.99]\n",
      "epoch: 2, batch: 100, loss is: [0.23558098], acc is [0.94]\n",
      "epoch: 2, batch: 200, loss is: [0.19376075], acc is [0.93]\n",
      "epoch: 2, batch: 300, loss is: [0.12970923], acc is [0.96]\n",
      "epoch: 2, batch: 400, loss is: [0.22810219], acc is [0.93]\n",
      "epoch: 3, batch: 0, loss is: [0.07354242], acc is [0.99]\n",
      "epoch: 3, batch: 100, loss is: [0.14670064], acc is [0.94]\n",
      "epoch: 3, batch: 200, loss is: [0.11205033], acc is [0.97]\n",
      "epoch: 3, batch: 300, loss is: [0.09048948], acc is [0.99]\n",
      "epoch: 3, batch: 400, loss is: [0.11145373], acc is [0.97]\n",
      "epoch: 4, batch: 0, loss is: [0.15413982], acc is [0.94]\n",
      "epoch: 4, batch: 100, loss is: [0.07696663], acc is [0.99]\n",
      "epoch: 4, batch: 200, loss is: [0.10113564], acc is [0.97]\n",
      "epoch: 4, batch: 300, loss is: [0.09746484], acc is [0.99]\n",
      "epoch: 4, batch: 400, loss is: [0.11896148], acc is [0.95]\n",
      "epoch: 5, batch: 0, loss is: [0.08308718], acc is [0.99]\n",
      "epoch: 5, batch: 100, loss is: [0.11196262], acc is [0.98]\n",
      "epoch: 5, batch: 200, loss is: [0.16062444], acc is [0.96]\n",
      "epoch: 5, batch: 300, loss is: [0.05889265], acc is [0.99]\n",
      "epoch: 5, batch: 400, loss is: [0.04552142], acc is [0.99]\n",
      "epoch: 6, batch: 0, loss is: [0.0973676], acc is [0.97]\n",
      "epoch: 6, batch: 100, loss is: [0.0657087], acc is [0.99]\n",
      "epoch: 6, batch: 200, loss is: [0.0503488], acc is [0.98]\n",
      "epoch: 6, batch: 300, loss is: [0.07743154], acc is [0.98]\n",
      "epoch: 6, batch: 400, loss is: [0.06850421], acc is [0.97]\n",
      "epoch: 7, batch: 0, loss is: [0.05562993], acc is [0.98]\n",
      "epoch: 7, batch: 100, loss is: [0.15631452], acc is [0.97]\n",
      "epoch: 7, batch: 200, loss is: [0.27259758], acc is [0.96]\n",
      "epoch: 7, batch: 300, loss is: [0.10099474], acc is [0.96]\n",
      "epoch: 7, batch: 400, loss is: [0.15799667], acc is [0.95]\n",
      "epoch: 8, batch: 0, loss is: [0.08646253], acc is [0.99]\n",
      "epoch: 8, batch: 100, loss is: [0.08697006], acc is [0.97]\n",
      "epoch: 8, batch: 200, loss is: [0.08223524], acc is [0.98]\n",
      "epoch: 8, batch: 300, loss is: [0.05165785], acc is [0.99]\n",
      "epoch: 8, batch: 400, loss is: [0.1486691], acc is [0.96]\n",
      "epoch: 9, batch: 0, loss is: [0.04463351], acc is [0.99]\n",
      "epoch: 9, batch: 100, loss is: [0.06625456], acc is [0.97]\n",
      "epoch: 9, batch: 200, loss is: [0.16728832], acc is [0.96]\n",
      "epoch: 9, batch: 300, loss is: [0.03115457], acc is [0.99]\n",
      "epoch: 9, batch: 400, loss is: [0.0700021], acc is [0.97]\n"
     ]
    }
   ],
   "source": [
    "#引入VisualDL库，并设定保存作图数据的文件位置\n",
    "from visualdl import LogWriter\n",
    "log_writer = LogWriter(logdir=\"./log\")\n",
    "\n",
    "with fluid.dygraph.guard(place):\n",
    "    model = MNIST()\n",
    "    model.train() \n",
    "    \n",
    "    optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.01, parameter_list=model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 10\n",
    "    iter = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            #准备数据，变得更加简洁\n",
    "            image_data, label_data = data\n",
    "            image = fluid.dygraph.to_variable(image_data)\n",
    "            label = fluid.dygraph.to_variable(label_data)\n",
    "            \n",
    "            #前向计算的过程，同时拿到模型输出值和分类准确率\n",
    "            predict, avg_acc = model(image, label)\n",
    "\n",
    "            #计算损失，取一个批次样本损失的平均值\n",
    "            loss = fluid.layers.cross_entropy(predict, label)\n",
    "            avg_loss = fluid.layers.mean(loss)\n",
    "            \n",
    "            #每训练了100批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), avg_acc.numpy()))\n",
    "                log_writer.add_scalar(tag = 'acc', step = iter, value = avg_acc.numpy())\n",
    "                log_writer.add_scalar(tag = 'loss', step = iter, value = avg_loss.numpy())\n",
    "                iter = iter + 100\n",
    "\n",
    "            #后向传播，更新参数的过程\n",
    "            avg_loss.backward()\n",
    "            optimizer.minimize(avg_loss)\n",
    "            model.clear_gradients()\n",
    "\n",
    "    #保存模型参数\n",
    "    fluid.save_dygraph(model.state_dict(), 'mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* 步骤3：命令行启动 VisualDL\n",
    "\n",
    "使用“visualdl --logdir [数据文件所在文件夹路径] 的命令启动VisualDL。在VisualDL启动后，命令行会打印出可用浏览器查阅图形结果的网址。\n",
    "``` \n",
    "$ visualdl --logdir ./log --port 8080\n",
    "```\n",
    "\n",
    "* 步骤4：打开浏览器，查看作图结果，如 **图6** 所示。\n",
    "\n",
    "查阅的网址在第三步的启动命令后会打印出来（如http://127.0.0.1:8080/），将该网址输入浏览器地址栏刷新页面的效果如下图所示。除了右侧对数据点的作图外，左侧还有一个控制板，可以调整诸多作图的细节。\n",
    "\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/c29fdeed4f404eeb846c8c864497f939b5920c3e6fea4f60a9e4ebe4503b6a6b\" width=\"600\" hegiht=\"\" ></center>\n",
    "<center><br>图6：visualdl的作图示例</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 作业题 2-4\n",
    "\n",
    "* 将普通神经网络模型的每层输出打印，观察内容。\n",
    "* 将分类准确率的指标 用PLT库画图表示。\n",
    "* 通过分类准确率，判断以采用不同损失函数训练模型的效果优劣。\n",
    "* 作图比较：随着训练进行，模型在训练集和测试集上的Loss曲线。\n",
    "* 调节正则化权重，观察4的作图曲线的变化，并分析原因。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.7.1 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
